{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f17561",
   "metadata": {},
   "source": [
    "# Practice Board\n",
    "\n",
    "We have been doing a lot of stuff around Multi Layer Perceptrons. Here I am gonna implement everything from scratch without using any auto complete and see if I am able to achieve the loss as I did in my previous notebooks.\n",
    "\n",
    "Answer these 2 questions:\n",
    "\n",
    "Q1. I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\n",
    "\n",
    "A1: If we initialize the weights and biases to zero, if our non-linear function is a sigmoid or tanh, the gradient will be zero for all the weights and biases. This will cause the network to not learn anything becuase all the neurons emit the SAME thing. We can assume tha only the output layer is the one for which the gradient is non-zero and hence that is the only thing that helps in training.\n",
    "\n",
    "Q2. BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1aac228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8615cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]\n",
    "\n",
    "# Build the vocabulary of characters\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fe1f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "emb_size = 10\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "n_hidden = 100\n",
    "batch_size = 32\n",
    "n_epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50875f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will create the dataset that is needed\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        init_pad = ['.'] * block_size\n",
    "        context = init_pad + list(word) + ['.']\n",
    "\n",
    "        for i in range(len(context) - block_size):\n",
    "            X.append([stoi[ch] for ch in context[i:i + block_size]])\n",
    "            Y.append(stoi[context[i + block_size]]) \n",
    "\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "# We will divide it into test, training and validation samples\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xval, Yval = build_dataset(words[n2:])\n",
    "\n",
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b856ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fan_in decides the input, fan_out decides the number of neurons in one layer.\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, gain=5/3):\n",
    "        # self.W = torch.randn((fan_in, fan_out), generator=g)\n",
    "        self.W = torch.zeros(fan_in * fan_out).reshape(fan_in, fan_out) / (gain * (fan_in ** 0.5))\n",
    "        self.B = torch.zeros(fan_out)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        # Now this layer needs to do wx + b\n",
    "        self.out = (X @ self.W) + self.B\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.B]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, X):\n",
    "        self.out = torch.tanh(X)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4215512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46497\n"
     ]
    }
   ],
   "source": [
    "# Why is the first layer's input this -> Linear(emb_size * block_size, n_hidden)?\n",
    "# let us assume we want to give 'e', 'm', 'm' to our first input layer. We cannot give 3 X 10 matrix as an input. We flatten it and give 30 embeddings, where each set of 10\n",
    "# represents the character.\n",
    "\n",
    "C = torch.randn((vocab_size, emb_size), generator=g)\n",
    "\n",
    "layers = [Linear(emb_size * block_size, n_hidden), Tanh(),\n",
    "Linear(n_hidden, n_hidden), Tanh(),\n",
    "Linear(n_hidden, n_hidden), Tanh(),\n",
    "Linear(n_hidden, n_hidden), Tanh(),\n",
    "Linear(n_hidden, n_hidden), Tanh(), \n",
    "Linear(n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "# This wont matter much becuase we are using batch norm\n",
    "with torch.no_grad():\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.W *= 5 / 3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5659e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 2.9232704639434814\n",
      "Loss at iteration 10000: 2.597027540206909\n",
      "Loss at iteration 20000: 2.596876859664917\n",
      "Loss at iteration 30000: 2.732625722885132\n",
      "Loss at iteration 40000: 2.9052674770355225\n",
      "Loss at iteration 50000: 2.735487937927246\n",
      "Loss at iteration 60000: 2.9485535621643066\n",
      "Loss at iteration 70000: 3.0972890853881836\n",
      "Loss at iteration 80000: 2.8423728942871094\n",
      "Loss at iteration 90000: 2.4960615634918213\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "backprops = []\n",
    "\n",
    "n_epochs = 100000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    activations = []\n",
    "    #First we create the minibatch.\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # Forward Pass\n",
    "    emb = C[Xb] # Advanced indexing operation\n",
    "    current_inp = emb.view(emb.shape[0], -1) # Flatten from the right till the 0th index.\n",
    "\n",
    "    for layer in layers:\n",
    "        current_inp = layer(current_inp)\n",
    "        if isinstance(layer, Linear):\n",
    "            activations.append(current_inp)\n",
    "\n",
    "    loss = F.cross_entropy(current_inp, Yb)\n",
    "\n",
    "    #Backward Pass\n",
    "    for params in parameters:\n",
    "        params.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update step\n",
    "    lr = 0.1 if i < 50000 else 0.01 # Learning rate decay for better convergence\n",
    "    for params in parameters:\n",
    "        params.data += -lr * params.grad\n",
    "        \n",
    "\n",
    "    # Stats\n",
    "    if i % 10000 == 0:\n",
    "        loss = F.cross_entropy(current_inp, Yb)\n",
    "        lossi.append(loss.item())\n",
    "        print(f\"Loss at iteration {i}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd36072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2870, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xval]\n",
    "current_inp = emb.view(emb.shape[0], -1) # Flatten from the right till the 0th index.\n",
    "\n",
    "for layer in layers:\n",
    "    current_inp = layer(current_inp)\n",
    "\n",
    "F.cross_entropy(current_inp, Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c475d",
   "metadata": {},
   "source": [
    "BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d123d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
