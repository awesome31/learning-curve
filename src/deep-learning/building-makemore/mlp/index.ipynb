{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3b2a3e",
   "metadata": {},
   "source": [
    "# MLP for Character Level Language Model\n",
    "\n",
    "In the last section we built a bigram model. In this section we are going to build a character level language model using MLP. Instead of considering ONLY the previous character, we are going to consider multiple characters. We are going to follow this paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "## Understanding the Model as described in the paper\n",
    "\n",
    "At a very high level, the model describes a vocabulary of 17000 words and is trained to learn the probability of each word given the 3 previous word in a sentence. It has 17000 output neurons, x amount of neurons in the hidden layer and a 30 X 1 vector for each word. Let us do the implementation of this model to understand the details. The idea is that through the embedding space, we are able to capture the semantic meaning of the words and use it to predict the next word in the sentence. Embedding space is a space where each word is represented by a vector. In this paper the vector's size is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e071e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b14a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "_words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "_words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de955890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary of characters\n",
    "_chars = sorted(list(set(''.join(_words))))\n",
    "_stoi = {s:i + 1 for i,s in enumerate(_chars)}\n",
    "_stoi['.'] = 0\n",
    "_itos = {i:s for s,i in _stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca805cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dataset\n",
    "_block_size = 3 # This is the context length of how many characters we use to predict the next character\n",
    "\n",
    "_X, _Y = [], []\n",
    "\n",
    "for _w in _words:\n",
    "    _context = ['.'] * _block_size\n",
    "    _w = _context + list(_w) + ['.']\n",
    "\n",
    "    for _i in range(len(_w) - _block_size):\n",
    "        _current_context = _w[_i:_i+_block_size]\n",
    "        _target = _w[_i+_block_size]\n",
    "        _X.append([_stoi[c] for c in _current_context])\n",
    "        _Y.append(_stoi[_target])\n",
    "\n",
    "_X = torch.tensor(_X)\n",
    "_Y = torch.tensor(_Y)\n",
    "_X.shape, _Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d1d88",
   "metadata": {},
   "source": [
    "Tensors should always be of floats or integer to feed to the model. The difference in our implementation is, that we are still predicting the characters instead of the words. In the paper the vocabulary size is 17000 and the embedding space is of size 17000 x 30. In our implementatio the vocubalry size is 27 and the embedding space we will create is 27 x 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba05b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C represents the embedding table space\n",
    "_C = torch.randn((27, 2))\n",
    "\n",
    "# Now that we have created the lookup table, we need a way to GET the embedding vector for a given character. These are the 2 ways.\n",
    "print(F.one_hot(torch.tensor(5), num_classes=27).float() @ _C)\n",
    "print(_C[5])\n",
    "\n",
    "# We will be using the second way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bfdab",
   "metadata": {},
   "source": [
    "To make plucking out values from the C map easy, let us understand how indexing works in pytorch. The below example shows that we can index a tensor using a list of indices easily. Using indexing we can input all of the Xs into the embedding table and get the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "_emb = _C[_X]\n",
    "_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bbc4d",
   "metadata": {},
   "source": [
    "Now lets construct the hidden layer. First let us understand the dimensions of the hidden layer.\n",
    "\n",
    "Let us think of 1 neuron. 1 neuron will get 3 characters (ch1, ch2, ch3) and each character has 2 dimensions of embedding space (c1, c2). Therefore for each neuron the equation will be:\n",
    "\n",
    "\n",
    "$Z = W_1 c_{11} + W_2 c_{12} + W_3 c_{21} + W_4 c_{22} + W_5 c_{31} + W_6 c_{32} + b$\n",
    "\n",
    "Therefore, the hidden layer's weights for 1 neuron will be of size 6 and the bias will be of size 1. The number of neurons in the hidden layer is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f172c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we construct the hidden layer\n",
    "_W1 = torch.randn((6, 100))\n",
    "_b1 = torch.randn((1, 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5152f5",
   "metadata": {},
   "source": [
    "Since our embedding is of the shape, 32 x 3 x 2, we need to flatten it to 32 x 6. We can use concat and unbind but there is much better and faster way to do it.\n",
    "\n",
    "In pytorch, we can use view to flatten the tensor. Internally every tensor in pytorch has something called a storage. The storage is the representation of the tensor in a 1D array. When we use view or print a tensor, the flat array of bytes remain unchanged but the internal representation of the tensor changes. With views, there is no copying of data and hence it is very fast. \n",
    "\n",
    "In our case, we want to just represent our 32 x 3 x 2 tensor as 32 x 6 tensor. We can use view to do this. Te view operation works right to left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6893cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try some torch operation\n",
    "print(_emb[:, 0, :].shape) # This will give us the embedding of the first character of each sequence.\n",
    "\n",
    "_a = torch.arange(8)\n",
    "print(_a)\n",
    "print(_a.view(2, 4)) # We can very easily reshape the tensor. .view is VERY efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9022010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the hidden layer 1\n",
    "_h = torch.tanh(_emb.view(-1, 6) @ _W1 + _b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_W2 = torch.randn(100, 27)\n",
    "_b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59495ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = _h @ _W2 + _b2\n",
    "_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can perform the softmax operation\n",
    "_counts = _logits.exp()\n",
    "_probs = _counts / _counts.sum(dim=1, keepdim=True)\n",
    "_loss = -_probs[torch.arange(_X.shape[0]), _Y].log().mean()\n",
    "\n",
    "_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a3ccf",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8c916",
   "metadata": {},
   "source": [
    "This piece of code is called the Cross Entropy loss.\n",
    "````\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(X.shape[0]), Y].log().mean()\n",
    "````\n",
    "\n",
    " We calculated this loss manually in the bigram model. But using pytorch's internal method has added advantages:\n",
    "\n",
    " 1. It does not create these intermediate tensors, which saves significant amount of memory.\n",
    " 2. The backward pass can be made much simpler.\n",
    " 3. If the logits are somehow very large or very small, we can run into numerical underflow and overflow which results in NaN operation when doing .exp(). Pytorch's implementation is more numerically stable. What PyTorch does and what we have learnt already is that we subtract all logit values with the maximum of the logits to create a value less than 1. This is called log-sum-exp trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1ef4c",
   "metadata": {},
   "source": [
    "### Optimising the Training Loop\n",
    "We can see that 100 iterations is taking roughly around 15 seconds to run the forward and the backward pass. To make gradient descent a little better, we can do minibatch gradient descent. In this we take a minibatch of data and then perform the forward pass.\n",
    "\n",
    "### Finding a good learning rate\n",
    "Having a good learning rate really will help the model train betetr and faster. For this we can create a linspace of learning rates and then perform gradient descent for each of them and see which one converges the fastest. Usually during training people like to do learning rate decay, i.e. reduce the learning rate after some epochs.\n",
    "\n",
    "### Splitting dataset\n",
    "As the capability of the neural network increases, becuase of the number of parameters, the NN is at the risk of something called overfitting. Overfitting means that it has trained or learnt the training data too well and is not able to generalise to new data. For this, We usually try to split the dataset into training, validation and test set. Training set is used to train the model, validation set is used to tune the hyperparameters and test set is used to evaluate the model.\n",
    "\n",
    "### Experimenting with large hidden layer outputs\n",
    "\n",
    "We can see that the hidden layer outputs are not that large. We can try to increase the hidden layer outputs to see if it helps the model to learn better. We bring the loss to roughly around 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194a0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "block_size = 3\n",
    "emb_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994a6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]\n",
    "\n",
    "# Build the vocabulary of characters\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d13c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3]) torch.Size([24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  0],\n",
       "         [ 0,  0, 20],\n",
       "         [ 0, 20,  1],\n",
       "         [20,  1, 21],\n",
       "         [ 1, 21, 18],\n",
       "         [21, 18,  5],\n",
       "         [18,  5, 14],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 19],\n",
       "         [ 0, 19, 21],\n",
       "         [19, 21, 12],\n",
       "         [21, 12,  5],\n",
       "         [12,  5, 13],\n",
       "         [ 5, 13,  1],\n",
       "         [13,  1, 14],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 26],\n",
       "         [ 0, 26,  5],\n",
       "         [26,  5, 18],\n",
       "         [ 5, 18,  5],\n",
       "         [18,  5, 14],\n",
       "         [ 5, 14,  9],\n",
       "         [14,  9, 20],\n",
       "         [ 9, 20, 25]]),\n",
       " tensor([20,  1, 21, 18,  5, 14,  0, 19, 21, 12,  5, 13,  1, 14,  0, 26,  5, 18,\n",
       "          5, 14,  9, 20, 25,  0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset(words):\n",
    "    # Creating the Dataset\n",
    "    block_size = 3 # This is the context length of how many characters we use to predict the next character\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words[:3]:\n",
    "        context = ['.'] * block_size\n",
    "        w = context + list(w) + ['.']\n",
    "\n",
    "        for i in range(len(w) - block_size):\n",
    "            current_context = w[i:i+block_size]\n",
    "            target = w[i+block_size]\n",
    "            X.append([stoi[c] for c in current_context])\n",
    "            Y.append(stoi[target])\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    # Let us create a more easy to read forward pass and loss function for the neural network.\n",
    "    # Every charactcer is encided in a vector of size 2\n",
    "    C = torch.randn((27, emb_size), generator=g, requires_grad=True)\n",
    "\n",
    "    W1 = torch.randn((block_size * emb_size, 200), generator=g, requires_grad=True)\n",
    "    b1 = torch.randn((1, 200), generator=g, requires_grad=True)\n",
    "\n",
    "\n",
    "    W2 = torch.randn((200, 27), generator=g, requires_grad=True)\n",
    "    b2 = torch.randn((1, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    print(torch.tensor([i.nelement() for i in[C, W1, b1, W2, b2]]).sum())\n",
    "    return C, W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(X, Y, lr, params):\n",
    "        # Training Loop\n",
    "    C, W1, b1, W2, b2 = params\n",
    "    for _ in range(50000):\n",
    "        # Forward Pass\n",
    "        ix = torch.randint(0, X.shape[0], (32,), generator=g) \n",
    "        emb = C[X[ix]]\n",
    "        h = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Y[ix])\n",
    "        # Backward Pass\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        lr = 0.1\n",
    "        for p in params:\n",
    "            p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade9afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(params, X, Y):\n",
    "    C, W1, b1, W2, b2 = params\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, block_size * emb_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf3f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182437, 3]) torch.Size([182437])\n",
      "torch.Size([22781, 3]) torch.Size([22781])\n",
      "torch.Size([22928, 3]) torch.Size([22928])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0]), tensor(20))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can create the training, validation and test splits.\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = get_dataset(words[:n1])\n",
    "Xdev, Ydev = get_dataset(words[n1:n2])\n",
    "Xte, Yte = get_dataset(words[n2:])\n",
    "\n",
    "Xtr[0], Ytr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    # Training using the training set\n",
    "    training_loop(Xtr, Ytr, 0.05, params)\n",
    "\n",
    "get_loss(params, Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b549d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_loss(params, Xdev, Ydev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba95b1e",
   "metadata": {},
   "source": [
    "### Visualizing the Character Embedding\n",
    "\n",
    "We can visualuze the character embedding since its a 2D vector. With this we can see what our character model has learnt. If we see the visualization, we can notice a few patterns:\n",
    "\n",
    "1. The vowels are close to each other which means that our neural network learnt that vowels are similar to each other.\n",
    "2. Characters like q or . have their own special embeddings and everything is clustered.\n",
    "\n",
    "Now we will scale up the size of our embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d475da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "C, W1, b1, W2, b2 = params\n",
    "plt.scatter(C[:, 0].data, C[:, 1].data, s=200)\n",
    "\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "plt.grid('minor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 10\n",
    "params = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(1):\n",
    "    # Training using the training set\n",
    "    training_loop(Xtr, Ytr, 0.05, params)\n",
    "\n",
    "get_loss(params, Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_loss(params, Xdev, Ydev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b801760",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Using the model we have trained, let us try and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab006c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 11,  5, 26, 10,  9,  8,  8,  1,  4, 12, 16,  1, 12,  7,  7, 11,  4,\n",
       "        17, 17, 18,  9,  2,  4,  8,  5, 24, 25, 20, 23,  9, 11])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 27, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ce424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
